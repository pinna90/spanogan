{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dxlab/jupyter/envs/gpu_taeyoung_py36/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import datetime\n",
    "from models.recurrent_models_pyramid import LSTMGenerator, LSTMDiscriminator\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgsTrn:\n",
    "    workers=4\n",
    "    batch_size=32\n",
    "    epochs=20\n",
    "    lr=0.0002\n",
    "    cuda = True\n",
    "    manualSeed=2\n",
    "    \n",
    "opt_trn=ArgsTrn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(opt_trn.manualSeed)\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('./X_train.npy')\n",
    "y_train = np.load('./y_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load('./X_test.npy')\n",
    "y_test = np.load('./y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:, :60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11000, 60, 1])\n",
      "torch.Size([11000])\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_train = torch.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "dataset = TensorDataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=opt_trn.batch_size,\n",
    "                                         shuffle=True, num_workers=int(opt_trn.workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if opt_trn.cuda else \"cpu\") # select the device\n",
    "seq_len = 60\n",
    "in_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generator and discriminator models\n",
    "netD = LSTMDiscriminator(in_dim=in_dim, device=device).to(device)\n",
    "netG = LSTMGenerator(in_dim=in_dim, out_dim=in_dim, device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Discriminator Architecture|\n",
      " LSTMDiscriminator(\n",
      "  (lstm): LSTM(1, 100, batch_first=True)\n",
      "  (linear): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n",
      "|Generator Architecture|\n",
      " LSTMGenerator(\n",
      "  (lstm0): LSTM(1, 32, batch_first=True)\n",
      "  (lstm1): LSTM(32, 64, batch_first=True)\n",
      "  (lstm2): LSTM(64, 128, batch_first=True)\n",
      "  (linear): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=1, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"|Discriminator Architecture|\\n\", netD)\n",
    "print(\"|Generator Architecture|\\n\", netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss function\n",
    "criterion = nn.BCELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=opt_trn.lr)\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=opt_trn.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dxlab/jupyter/envs/gpu_taeyoung_py36/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "/home/dxlab/jupyter/envs/gpu_taeyoung_py36/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/20][343/344] Loss_D: 1.3779 Loss_G: 0.6405 D(x): 0.5360 D(G(z)): 0.5286 / 0.5278\n",
      "[1/20][343/344] Loss_D: 1.2641 Loss_G: 0.5501 D(x): 0.6719 D(G(z)): 0.5777 / 0.5774\n",
      "[2/20][343/344] Loss_D: 0.8388 Loss_G: 1.6729 D(x): 0.5945 D(G(z)): 0.2372 / 0.2366\n",
      "[3/20][343/344] Loss_D: 1.5179 Loss_G: 0.3859 D(x): 0.6909 D(G(z)): 0.6810 / 0.6807\n",
      "[4/20][343/344] Loss_D: 1.4769 Loss_G: 0.4348 D(x): 0.6492 D(G(z)): 0.6477 / 0.6477\n",
      "[5/20][343/344] Loss_D: 1.4995 Loss_G: 0.4166 D(x): 0.6574 D(G(z)): 0.6597 / 0.6596\n",
      "[6/20][343/344] Loss_D: 1.4809 Loss_G: 0.4266 D(x): 0.6568 D(G(z)): 0.6531 / 0.6530\n",
      "[7/20][343/344] Loss_D: 1.5032 Loss_G: 0.4078 D(x): 0.6665 D(G(z)): 0.6656 / 0.6655\n",
      "[8/20][343/344] Loss_D: 1.4970 Loss_G: 0.4165 D(x): 0.6588 D(G(z)): 0.6597 / 0.6596\n",
      "[9/20][343/344] Loss_D: 1.5033 Loss_G: 0.4080 D(x): 0.6659 D(G(z)): 0.6654 / 0.6652\n",
      "[10/20][343/344] Loss_D: 1.4925 Loss_G: 0.4190 D(x): 0.6585 D(G(z)): 0.6581 / 0.6579\n",
      "[11/20][343/344] Loss_D: 1.4970 Loss_G: 0.4151 D(x): 0.6604 D(G(z)): 0.6607 / 0.6605\n",
      "[12/20][343/344] Loss_D: 1.4990 Loss_G: 0.4132 D(x): 0.6614 D(G(z)): 0.6619 / 0.6617\n",
      "[13/20][343/344] Loss_D: 1.4943 Loss_G: 0.4168 D(x): 0.6599 D(G(z)): 0.6596 / 0.6593\n",
      "[14/20][343/344] Loss_D: 1.5128 Loss_G: 0.3996 D(x): 0.6705 D(G(z)): 0.6710 / 0.6708\n",
      "[15/20][343/344] Loss_D: 1.4958 Loss_G: 0.4126 D(x): 0.6642 D(G(z)): 0.6622 / 0.6621\n",
      "[16/20][343/344] Loss_D: 1.4924 Loss_G: 0.4159 D(x): 0.6620 D(G(z)): 0.6600 / 0.6599\n",
      "[17/20][343/344] Loss_D: 1.4971 Loss_G: 0.4126 D(x): 0.6631 D(G(z)): 0.6622 / 0.6621\n",
      "[18/20][343/344] Loss_D: 1.4958 Loss_G: 0.4158 D(x): 0.6598 D(G(z)): 0.6601 / 0.6599\n",
      "[19/20][343/344] Loss_D: 1.4964 Loss_G: 0.4110 D(x): 0.6656 D(G(z)): 0.6632 / 0.6631\n"
     ]
    }
   ],
   "source": [
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "for epoch in range(opt_trn.epochs):\n",
    "    for i, (x,y) in enumerate(dataloader, 0):\n",
    "        \n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "\n",
    "        #Train with real data\n",
    "        netD.zero_grad()\n",
    "        real = x.to(device)\n",
    "        batch_size, seq_len = real.size(0), real.size(1)\n",
    "        label = torch.full((batch_size, seq_len, 1), real_label, device=device)\n",
    "\n",
    "        output,_ = netD.forward(real)\n",
    "        errD_real = criterion(output, label.float())\n",
    "        errD_real.backward()\n",
    "        optimizerD.step()\n",
    "        D_x = output.mean().item()\n",
    "        \n",
    "        #Train with fake data\n",
    "        noise = Variable(init.normal(torch.Tensor(batch_size,seq_len,in_dim),mean=0,std=0.1)).cuda()\n",
    "        fake,_ = netG.forward(noise)\n",
    "        output,_ = netD.forward(fake.detach()) # detach causes gradient is no longer being computed or stored to save memeory\n",
    "        label.fill_(fake_label)\n",
    "        errD_fake = criterion(output, label.float())\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "        \n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        noise = Variable(init.normal(torch.Tensor(batch_size,seq_len,in_dim),mean=0,std=0.1)).cuda()\n",
    "        fake,_ = netG.forward(noise)\n",
    "        label.fill_(real_label) \n",
    "        output,_ = netD.forward(fake)\n",
    "        errG = criterion(output, label.float())\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        \n",
    "        \n",
    "\n",
    "    print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f' \n",
    "          % (epoch, opt_trn.epochs, i, len(dataloader),\n",
    "             errD.item(), errG.item(), D_x, D_G_z1, D_G_z2), end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgsTest:\n",
    "    workers = 1\n",
    "    batch_size = 1\n",
    "    \n",
    "opt_test=ArgsTest() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = netG # changing reference variable \n",
    "discriminator = netD # changing reference variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load('./X_test.npy')\n",
    "y_test = np.load('./y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test[:, :60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11000, 60, 1])\n",
      "torch.Size([11000])\n"
     ]
    }
   ],
   "source": [
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "X_test = torch.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "test_dataset = TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.,  ..., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=opt_trn.batch_size,\n",
    "                                         shuffle=True, num_workers=int(opt_trn.workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda = 0.1 according to paper\n",
    "# x is new data, G_z is closely regenerated data\n",
    "\n",
    "def Anomaly_score(x, G_z, Lambda=0.1):\n",
    "    residual_loss = torch.sum(torch.abs(x-G_z)) # Residual Loss\n",
    "    \n",
    "    # x_feature is a rich intermediate feature representation for real data x\n",
    "    output, x_feature = discriminator(x.to(device)) \n",
    "    # G_z_feature is a rich intermediate feature representation for fake data G(z)\n",
    "    output, G_z_feature = discriminator(G_z.to(device)) \n",
    "    \n",
    "    discrimination_loss = torch.sum(torch.abs(x_feature-G_z_feature)) # Discrimination loss\n",
    "    \n",
    "    total_loss = (1-Lambda)*residual_loss.to(device) + Lambda*discrimination_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dxlab/jupyter/envs/gpu_taeyoung_py36/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~loss=136.6690673828125,  y=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) ~~~~~~~~~~\n",
      "1 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "~~~~~~~~loss=133.79026794433594,  y=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) ~~~~~~~~~~\n",
      "2 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "~~~~~~~~loss=131.47897338867188,  y=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) ~~~~~~~~~~\n",
      "3 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "~~~~~~~~loss=137.98959350585938,  y=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) ~~~~~~~~~~\n",
      "4 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "~~~~~~~~loss=137.34133911132812,  y=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) ~~~~~~~~~~\n",
      "5 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "~~~~~~~~loss=131.44345092773438,  y=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) ~~~~~~~~~~\n",
      "6 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "~~~~~~~~loss=135.63377380371094,  y=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) ~~~~~~~~~~\n",
      "7 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "~~~~~~~~loss=135.9999237060547,  y=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) ~~~~~~~~~~\n",
      "8 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "~~~~~~~~loss=133.06491088867188,  y=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) ~~~~~~~~~~\n",
      "9 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "~~~~~~~~loss=136.3581085205078,  y=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) ~~~~~~~~~~\n",
      "10 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "~~~~~~~~loss=134.68142700195312,  y=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) ~~~~~~~~~~\n",
      "11 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-61ce0227c4c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mgen_fake\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAnomaly_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mz_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter/envs/gpu_taeyoung_py36/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter/envs/gpu_taeyoung_py36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "#y_list = []\n",
    "\n",
    "batch_size = 32\n",
    "window_length = 60\n",
    "n_feature = 1\n",
    "\n",
    "for i, (x,y) in enumerate(test_dataloader):\n",
    "    print(i, y)\n",
    "    \n",
    "    #z = Variable(init.normal(torch.zeros(opt_test.batch_size,\n",
    "    #                                 test_dataset.window_length, \n",
    "    #                                 test_dataset.n_feature),mean=0,std=0.1),requires_grad=True)\n",
    "    #z = x\n",
    "    \n",
    "    z = Variable(init.normal(torch.zeros(batch_size,\n",
    "                                     window_length, \n",
    "                                     n_feature),mean=0,std=0.1),requires_grad=True) \n",
    "    \n",
    "    z_optimizer = torch.optim.Adam([z],lr=1e-2)\n",
    "    \n",
    "    loss = None\n",
    "    for j in range(50): # set your interation range\n",
    "        gen_fake,_ = generator(z.cuda())\n",
    "        loss = Anomaly_score(Variable(x).cuda(), gen_fake)\n",
    "        loss.backward()\n",
    "        z_optimizer.step()\n",
    "\n",
    "    loss_list.append(loss) # Store the loss from the final iteration\n",
    "    #y_list.append(y) # Store the corresponding anomaly label\n",
    "    print('~~~~~~~~loss={},  y={} ~~~~~~~~~~'.format(loss, y))\n",
    "    #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "THRESHOLD = 2.0 # Anomaly score threshold for an instance to be considered as anomaly \n",
    "\n",
    "#TIME_STEPS = dataset.window_length\n",
    "#test_score_df = pd.DataFrame(index=range(test_dataset.data_len))\n",
    "test_score_df = pd.DataFrame(index=range(len(loss_list)))\n",
    "#test_score_df['loss'] = [loss.item()/test_dataset.window_length for loss in loss_list]\n",
    "test_score_df['loss'] = [loss.item()/window_length for loss in loss_list]\n",
    "test_score_df['y'] = test_dataset.tensors[1][:len(loss_list)]\n",
    "test_score_df['threshold'] = THRESHOLD\n",
    "test_score_df['anomaly'] = test_score_df.loss > test_score_df.threshold\n",
    "test_score_df['t'] = [x[59].item() for x in test_dataset.tensors[0][:len(loss_list)]]\n",
    "\n",
    "plt.plot(test_score_df.index, test_score_df.loss, label='loss')\n",
    "plt.plot(test_score_df.index, test_score_df.threshold, label='threshold')\n",
    "plt.plot(test_score_df.index, test_score_df.y, label='y')\n",
    "plt.xticks(rotation=25)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=opt_trn.batch_size,\n",
    "                                         shuffle=True, num_workers=int(opt_trn.workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda = 0.1 according to paper\n",
    "# x is new data, G_z is closely regenerated data\n",
    "\n",
    "def Anomaly_score(x, G_z, Lambda=0.1):\n",
    "    residual_loss = torch.sum(torch.abs(x-G_z)) # Residual Loss\n",
    "    \n",
    "    # x_feature is a rich intermediate feature representation for real data x\n",
    "    output, x_feature = discriminator(x.to(device)) \n",
    "    # G_z_feature is a rich intermediate feature representation for fake data G(z)\n",
    "    output, G_z_feature = discriminator(G_z.to(device)) \n",
    "    \n",
    "    discrimination_loss = torch.sum(torch.abs(x_feature-G_z_feature)) # Discrimination loss\n",
    "    \n",
    "    total_loss = (1-Lambda)*residual_loss.to(device) + Lambda*discrimination_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "#y_list = []\n",
    "\n",
    "batch_size = 32\n",
    "window_length = 60\n",
    "n_feature = 1\n",
    "\n",
    "for i, (x,y) in enumerate(dataloader):\n",
    "    print(i, y)\n",
    "    \n",
    "    #z = Variable(init.normal(torch.zeros(opt_test.batch_size,\n",
    "    #                                 test_dataset.window_length, \n",
    "    #                                 test_dataset.n_feature),mean=0,std=0.1),requires_grad=True)\n",
    "    #z = x\n",
    "    \n",
    "    z = Variable(init.normal(torch.zeros(batch_size,\n",
    "                                     window_length, \n",
    "                                     n_feature),mean=0,std=0.1),requires_grad=True) \n",
    "    \n",
    "    z_optimizer = torch.optim.Adam([z],lr=1e-2)\n",
    "    \n",
    "    loss = None\n",
    "    for j in range(50): # set your interation range\n",
    "        gen_fake,_ = generator(z.cuda())\n",
    "        loss = Anomaly_score(Variable(x).cuda(), gen_fake)\n",
    "        loss.backward()\n",
    "        z_optimizer.step()\n",
    "\n",
    "    loss_list.append(loss) # Store the loss from the final iteration\n",
    "    #y_list.append(y) # Store the corresponding anomaly label\n",
    "    print('~~~~~~~~loss={},  y={} ~~~~~~~~~~'.format(loss, y))\n",
    "    #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "THRESHOLD = 3.0 # Anomaly score threshold for an instance to be considered as anomaly \n",
    "\n",
    "#TIME_STEPS = dataset.window_length\n",
    "#test_score_df = pd.DataFrame(index=range(test_dataset.data_len))\n",
    "test_score_df = pd.DataFrame(index=range(len(loss_list)))\n",
    "#test_score_df['loss'] = [loss.item()/test_dataset.window_length for loss in loss_list]\n",
    "test_score_df['loss'] = [loss.item()/window_length for loss in loss_list]\n",
    "test_score_df['y'] = dataset.tensors[1][:len(loss_list)]\n",
    "test_score_df['threshold'] = THRESHOLD\n",
    "test_score_df['anomaly'] = test_score_df.loss > test_score_df.threshold\n",
    "test_score_df['t'] = [x[59].item() for x in dataset.tensors[0][:len(loss_list)]]\n",
    "\n",
    "plt.plot(test_score_df.index, test_score_df.loss, label='loss')\n",
    "plt.plot(test_score_df.index, test_score_df.threshold, label='threshold')\n",
    "plt.plot(test_score_df.index, test_score_df.y, label='y')\n",
    "plt.xticks(rotation=25)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_taeyoung_py36",
   "language": "python",
   "name": "gpu_taeyoung_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
